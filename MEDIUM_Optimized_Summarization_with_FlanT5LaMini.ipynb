{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrdAXbAevDlOhYKc6gyplq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91864d6f332c46ba839927acc49ead9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Restart Runtime",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_7cf4370e86d84291922ebe415e5bf73f",
            "style": "IPY_MODEL_0c0bd0b723504460a38ab75d41238a55",
            "tooltip": "Click me"
          }
        },
        "7cf4370e86d84291922ebe415e5bf73f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0bd0b723504460a38ab75d41238a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8f001b615787411d922cdbdd40773670": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6d86eb60d55446838920daa019b3e137",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m   0%\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:23\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   0%</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">0/1 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:23</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span> , <span style=\"color: #800000; text-decoration-color: #800000\">? it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "6d86eb60d55446838920daa019b3e137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5390469925c7401d9427f931395a0a91": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_bb63ffcf294d4708bf28f2064a97bd23",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m  50%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2 \u001b[0m [ \u001b[33m0:01:28\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  50%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">1/2 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:01:28</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span> , <span style=\"color: #800000; text-decoration-color: #800000\">? it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "bb63ffcf294d4708bf28f2064a97bd23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiomatricardi/Abstractive-Extractive/blob/main/MEDIUM_Optimized_Summarization_with_FlanT5LaMini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://uploads-ssl.webflow.com/5fdc17d51dc102ed1cf87c05/628cf8c014925ccdbc358d30_dengqingxiong_0-1585676096532.jpeg\" height=300>\n"
      ],
      "metadata": {
        "id": "xiYfGI3faWra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimize Summarizations with with_FlanT5LaMini\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Using Page Plain Text Chrome addon to download plain text from the Medium article\n",
        "Load it on Google Colab\n",
        "Use FlanT5-LaMini for Summarization and chat?"
      ],
      "metadata": {
        "id": "Z4e_7ABOMoB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL NOTEBOOK"
      ],
      "metadata": {
        "id": "hxYPT51iesjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/logo.png\">"
      ],
      "metadata": {
        "id": "YkVq79ON1cTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🏡 🦙 Test with locally downloaded model\n",
        "\n",
        "refer for text to https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09\n"
      ],
      "metadata": {
        "id": "AhO7RfFStfLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install mkl mkl-include\n",
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
        "!pip install git+https://github.com//huggingface/transformers\n",
        "!pip install accelerate\n",
        "!pip install rich"
      ],
      "metadata": {
        "id": "pZyIwQ2UBnZ5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install Cython\n",
        "!pip install tiktoken\n",
        "!pip install unstructured[local-inference]"
      ],
      "metadata": {
        "id": "kD6H0uBzrX-w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart Runtime {display-mode: \"form\"}\n",
        "import ipywidgets as widgets\n",
        "def restart(b):\n",
        "  exit()\n",
        "\n",
        "button2 = widgets.Button(\n",
        "    description='Restart Runtime',\n",
        "    disabled=False,\n",
        "    button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "button2.on_click(restart)\n",
        "button2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "91864d6f332c46ba839927acc49ead9c",
            "7cf4370e86d84291922ebe415e5bf73f",
            "0c0bd0b723504460a38ab75d41238a55"
          ]
        },
        "outputId": "3f6e0bb3-11cf-4c51-fe76-e456fd610e70",
        "id": "r0r7ikjKHqMV"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='Restart Runtime', icon='check', style=ButtonStyle(), tooltip='Clic…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91864d6f332c46ba839927acc49ead9c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download text files"
      ],
      "metadata": {
        "id": "4u-WHlxn7E5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/BERTexplanation.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/Text%20Summarization%20with%20NLP-%20TextRank%20vs%20Seq2Seq%20vs%20BART.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/AutomaticTextSummarization.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/GPT4VsLima.txt\n",
        "!wget https://github.com/fabiomatricardi/Abstractive-Extractive/raw/main/nlp-basics-abstractive-and-extractive-text-summarization.txt"
      ],
      "metadata": {
        "id": "5Ba9cKME7E6A"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Model from HuggingFace *MBZUAI/LaMini-Flan-T5-248M* and move it to Model folder"
      ],
      "metadata": {
        "id": "MMToFuhdMdkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/.gitattributes\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/.gitignore\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/README.md\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/config.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/generation_config.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/pytorch_model.bin\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/special_tokens_map.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/spiece.model\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/tokenizer.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/tokenizer_config.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/training_args.bin\n",
        "!mkdir model\n",
        "!mv /content/.gitattributes /content/model/.gitattributes\n",
        "!mv /content/.gitignore /content/model/.gitignore\n",
        "!mv /content/README.md /content/model/README.md\n",
        "!mv /content/config.json  /content/model/config.json\n",
        "!mv /content/generation_config.json  /content/model/generation_config.json\n",
        "!mv /content/pytorch_model.bin  /content/model/pytorch_model.bin\n",
        "!mv /content/special_tokens_map.json  /content/model/special_tokens_map.json\n",
        "!mv /content/spiece.model  /content/model/spiece.model\n",
        "!mv /content/tokenizer.json  /content/model/tokenizer.json\n",
        "!mv /content/tokenizer_config.json  /content/model/tokenizer_config.json\n",
        "!mv /content/training_args.bin  /content/model/training_args.bin"
      ],
      "metadata": {
        "id": "JEYKg_j7K0bC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize calls for Local T5 LaMini Model"
      ],
      "metadata": {
        "id": "GVYs_SDtrlyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, T5Tokenizer, T5Model\n",
        "from transformers import T5ForConditionalGeneration, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "import rich\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich import print\n",
        "import ssl\n",
        "import requests\n",
        "import datetime\n",
        "import os\n",
        "import requests\n",
        "import datetime\n",
        "#from tqdm import tqdm\n",
        "from tqdm.rich import trange, tqdm\n",
        "\n",
        "########### SSL FOR PROXY ##############\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "########## INITIALIZE RICH CONSOLE  ##################\n",
        "console = Console()\n",
        "\n",
        "\n",
        "########## LANGUAGE MODELS CHECKPOINTS ###########################\n",
        "checkpoint = \"./content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine"
      ],
      "metadata": {
        "id": "LkJSfonxEEHP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐ BEST PIPELINE settings chunks/overlap\n",
        "## ⭐ Langchain/pipelines chunk 600/100 good below 5000 character\n",
        "### ⭐ above 5000 characther 1700/300 or even 2000/300"
      ],
      "metadata": {
        "id": "suX9fvlcblCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################### Import TEXT file as String ###################\n",
        "fname = '/content/GPT4VsLima.txt'\n",
        "with open(fname) as f:\n",
        "    doc = f.read()\n",
        "f.close()\n",
        "#################### Assign CheckPoint path for Model ###################\n",
        "LaMini = '/content/model/'"
      ],
      "metadata": {
        "id": "sAn84vWab3Ms"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a small text from the GPT4VsLima..."
      ],
      "metadata": {
        "id": "oObhxAb9_oGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = \"\"\"Title: GPT-4 vs LIMA: Rethinking Large Language Models for Efficiency and Performance | by Amir Shakiba | May, 2023 | Medium\n",
        "GPT-4 vs LIMA: Rethinking Large Language Models for Efficiency and Performance\n",
        "written by Amir Shakiba\n",
        "A recent paper by Meta AI has the potential to revolutionize our understanding of large language models (LLMs).\n",
        "To delve into their workings, let’s take a closer look at Meta AI’s LLAMA model. (you can jump straight to LIMA if you want)\n",
        "LLAMA\n",
        "LLMs, which are trained on vast amounts of text, have given us impressive results. Initially, it was believed that bigger models are necessary for better performance. However, recent papers suggest that smaller models trained on more data can actually deliver better results, challenging the notion of model size. Importantly, practical considerations come into play. In terms of production efficiency, it is more advantageous to train a smaller model for a longer duration, rather than opting for a larger model trained in a shorter time frame that requires more GPU resources during inference.\n",
        "smaller models on larger datasets means less cost and more affordability leading to democratization of AI which OpenAI is really concerned about!\n",
        "This is where LLAMA models come in. Despite having fewer parameters compared to GPT-3 models, LLAMA models can run on a single GPU. Additionally, LLAMA models are exclusively trained on openly accessible datasets, in contrast to other systems like ChatGPT, which rely on data that is not publicly available.(openAI or closeAI?:)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tu2GY2NF_J4p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc2.split(' ')) #number of words in docs2\n",
        "len(doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR4AFoIY_JRb",
        "outputId": "8152459f-571b-4fa4-e28a-f2e5fa21d1fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1542"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "#     SUMMARIZATION FROM TEXT STRING WITH HUGGINGFACE PIPELINE       #\n",
        "######################################################################\n",
        "def AI_SummaryPL(checkpoint, text, chunks, overlap):\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, T5Tokenizer, T5Model\n",
        "    from transformers import T5ForConditionalGeneration, pipeline\n",
        "    from langchain.llms import HuggingFacePipeline\n",
        "    import torch\n",
        "\n",
        "    \"\"\"\n",
        "    checkpoint is in the format of relative path\n",
        "    example:  checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "    text it is either a long string or a input long string or a loaded document into string\n",
        "    chunks: integer, lenght of the chunks splitting\n",
        "    ovelap: integer, overlap for cor attention and focus retreival\n",
        "\n",
        "    \"\"\"\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = chunks,\n",
        "        chunk_overlap  = overlap,\n",
        "        length_function = len,\n",
        "    )\n",
        "    texts = text_splitter.split_text(text)\n",
        "    #checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "    checkpoint = checkpoint\n",
        "    tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(checkpoint,\n",
        "                                                        device_map='auto',\n",
        "                                                        torch_dtype=torch.float32)\n",
        "    console.print(\"[yellow bold] Inizializing pipeline\")\n",
        "    pipe_sum = pipeline('summarization',\n",
        "                        model = base_model,\n",
        "                        tokenizer = tokenizer,\n",
        "                        max_length = 650, #origina  350\n",
        "                        min_length = 25\n",
        "                        )\n",
        "\n",
        "    start = datetime.datetime.now() #not used now but useful\n",
        "    # taking from ipyWidget textarea content\n",
        "    full_summary = ''\n",
        "    for cnk in trange(len(texts)):\n",
        "      result = pipe_sum(texts[cnk])\n",
        "      full_summary = full_summary + ' '+ result[0]['summary_text']\n",
        "    stop = datetime.datetime.now() #not used now but useful\n",
        "    delta = stop-start\n",
        "\n",
        "    print(Panel(full_summary, title='AI Summarization'))\n",
        "    console.print(f\"[red bold]Summarization completed in {delta}\")\n",
        "    console.print(f\"Entire Summarization lenght: [bold bright_magenta]{len(full_summary)}[/bold bright_magenta]/{len(text)} character\")\n",
        "    reduction = '{:.1%}'.format(len(full_summary)/len(text))\n",
        "    console.print(f\"Summarization reduction: [bold bright_magenta]{reduction}\")\n",
        "    return full_summary\n"
      ],
      "metadata": {
        "id": "88yIQYvOb3jq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTRACTIVE EXAMPLE 1"
      ],
      "metadata": {
        "id": "gXd34SAuCbNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_summary14 = AI_SummaryPL(LaMini,doc2,1700,20)  #3700/500 for texts > 6000 charachters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "8f001b615787411d922cdbdd40773670",
            "6d86eb60d55446838920daa019b3e137"
          ]
        },
        "id": "Goyb37WNZjqU",
        "outputId": "96eb33d9-2284-4b07-dc4b-cbba62cb864e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing pipeline\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing pipeline</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f001b615787411d922cdbdd40773670"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/rich.py:145: TqdmExperimentalWarning: rich is experimental/alpha\n",
            "  return tqdm_rich(range(*args), **kwargs)\n",
            "Your max_length is set to 650, but your input_length is only 358. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=179)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│  Meta AI's LLAMA model, which is trained on vast amounts of text, has shown impressive results. However, recent │\n",
              "│ papers suggest that smaller models trained on more data can deliver better results, challenging the notion of   │\n",
              "│ model size. Smaller models can run on a single GPU and are exclusively trained on openly accessible datasets,   │\n",
              "│ leading to democratization of AI. OpenAI is concerned about this.                                               │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│  Meta AI's LLAMA model, which is trained on vast amounts of text, has shown impressive results. However, recent │\n",
              "│ papers suggest that smaller models trained on more data can deliver better results, challenging the notion of   │\n",
              "│ model size. Smaller models can run on a single GPU and are exclusively trained on openly accessible datasets,   │\n",
              "│ leading to democratization of AI. OpenAI is concerned about this.                                               │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:23\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m948627\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:23.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">948627</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Entire Summarization lenght: \u001b[1;95m397\u001b[0m/\u001b[1;36m1542\u001b[0m character\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entire Summarization lenght: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">397</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1542</span> character\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summarization reduction: \u001b[1;95m25.7\u001b[0m\u001b[1;95m%\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summarization reduction: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">25.7</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">%</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(post_summary14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "KK-KYoVDAu93",
        "outputId": "8bd53912-441a-4462-f8d4-197cc0c00786"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " Meta AI's LLAMA model, which is trained on vast amounts of text, has shown impressive results. However, recent \n",
              "papers suggest that smaller models trained on more data can deliver better results, challenging the notion of model\n",
              "size. Smaller models can run on a single GPU and are exclusively trained on openly accessible datasets, leading to \n",
              "democratization of AI. OpenAI is concerned about this.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Meta AI's LLAMA model, which is trained on vast amounts of text, has shown impressive results. However, recent \n",
              "papers suggest that smaller models trained on more data can deliver better results, challenging the notion of model\n",
              "size. Smaller models can run on a single GPU and are exclusively trained on openly accessible datasets, leading to \n",
              "democratization of AI. OpenAI is concerned about this.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTRACTIVE EXAMPLE 2"
      ],
      "metadata": {
        "id": "aISlqeKQCXzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 =\"\"\"\n",
        "BERT: A Beginner-Friendly Explanation written By Pushpam Punjabi\n",
        "Up until now, we’ve seen how a computer understands the meaning of different words using word embeddings. In the last blog, we also looked at how we can take average of the embeddings of words appearing in a sentence to represent that sentence as an embedding. This is one of the ways of interpreting a sentence. But that’s not how humans understand the language. We don’t just take individual meaning of words and form the understanding of a sentence or a paragraph. A much more complex process is involved to understand language by humans. But how does a machine understand language? It’s through language models!\n",
        "Language models are an essential component of Natural Language Processing (NLP), designed to understand and generate human language. They use various statistical and machine learning techniques to analyze and learn from large amounts of text data, enabling them to identify patterns and relationships between words, phrases, and sentences. Word embeddings form the base in understanding these sentences! Language models have revolutionized the field of NLP and have played a crucial role in enabling machines to interact with humans in a more natural and intuitive way. Language models have also surpassed humans in some of the tasks in NLP!\n",
        "In this blog, we will understand Bi-directional Encoder Representations from Transformers (BERT) which is one of the biggest milestones in the world on language models!\n",
        "Understanding BERT: BERT was developed by Google in 2018. It is a “Language Understanding” model, that is trained on a massive amounts of text data to understand the context and meaning of words and phrases in a sentence. BERT uses “transformer” deep learning architecture that enables it to process information bidirectionally, meaning it can understand the context of a word based on both, the words that come before and after it. This allows BERT to better understand the nuances of language, including idioms, sarcasm, and complex sentence structures.\n",
        "You must be wondering how do you train such models to understand human language? There are 2 training steps involved to use BERT:\n",
        "Pre-training phase\n",
        "Fine-tuning phase\n",
        "1. Pre-training phase\n",
        "In pre-training phase, the model is trained on huge textual data. This is the stage where the model learns and understand the language. Pre-training is expensive. To pre-train a BERT model, Google used multiple TPUs — special computing processors for deep learning models. It took them 4 days to pre-train BERT on such a large infrastructure. But this is only a one-time procedure. Once the model understands the language, we can reuse the model for variety of tasks in NLP. There are 3 steps to pre-train BERT:\n",
        "Text corpus selection\n",
        "Masked Language Modeling\n",
        "Next Sentence Prediction\n",
        "Let’s go through each step in detail.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sUnqmShfChLx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQslgbMEDKlq",
        "outputId": "5620f322-b7db-4b61-eddf-035203b0259c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2860"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_summary15 = AI_SummaryPL(LaMini,doc3,2000,200)  #3700/500 for texts > 6000 charachters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "5390469925c7401d9427f931395a0a91",
            "bb63ffcf294d4708bf28f2064a97bd23"
          ]
        },
        "id": "hGRnPoNeDIlS",
        "outputId": "a5f075f6-683e-4fd8-a0a4-362a994d728d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing pipeline\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing pipeline</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5390469925c7401d9427f931395a0a91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/rich.py:145: TqdmExperimentalWarning: rich is experimental/alpha\n",
            "  return tqdm_rich(range(*args), **kwargs)\n",
            "Your max_length is set to 650, but your input_length is only 319. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=159)\n",
            "Your max_length is set to 650, but your input_length is only 359. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=179)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│  BERT: A Beginner-Friendly Explanation by Pushpam Punjabi explains how a machine understands language through   │\n",
              "│ language models, which are essential components of Natural Language Processing (NLP) designed to understand and │\n",
              "│ generate human language. They use statistical and machine learning techniques to analyze and learn from large   │\n",
              "│ amounts of text data, enabling them to identify patterns and relationships between words, phrases, and          │\n",
              "│ sentences. Language models have revolutionized NLP and have surpassed humans in some of the tasks in NLP. The   │\n",
              "│ blog discusses the Bi-directional Encoder Representations from Transformers (BERT), a language model developed  │\n",
              "│ by Google in 2018. The model is trained on massive amounts of text data to understand the context and meaning   │\n",
              "│ of words and phrases in a sentence. BERT uses transformer deep learning architecture to process information     │\n",
              "│ bidirectionally, allowing it to better understand the nuances of language, including idioms, sarcasm, and       │\n",
              "│ complex sentence structures. Pre-training phase involves training the model on huge textual data, which is the  │\n",
              "│ stage where the model learns and understands the language. Google used multiple TPUs to pre-train a BERT model, │\n",
              "│ which took 4 days to train on such a large infrastructure. After training, the model can be reused for various  │\n",
              "│ tasks in NLP. There are three steps involved in pre-training BERT: Text corpus selection, Masked Language       │\n",
              "│ Modeling, Next Sentence Prediction.                                                                             │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│  BERT: A Beginner-Friendly Explanation by Pushpam Punjabi explains how a machine understands language through   │\n",
              "│ language models, which are essential components of Natural Language Processing (NLP) designed to understand and │\n",
              "│ generate human language. They use statistical and machine learning techniques to analyze and learn from large   │\n",
              "│ amounts of text data, enabling them to identify patterns and relationships between words, phrases, and          │\n",
              "│ sentences. Language models have revolutionized NLP and have surpassed humans in some of the tasks in NLP. The   │\n",
              "│ blog discusses the Bi-directional Encoder Representations from Transformers (BERT), a language model developed  │\n",
              "│ by Google in 2018. The model is trained on massive amounts of text data to understand the context and meaning   │\n",
              "│ of words and phrases in a sentence. BERT uses transformer deep learning architecture to process information     │\n",
              "│ bidirectionally, allowing it to better understand the nuances of language, including idioms, sarcasm, and       │\n",
              "│ complex sentence structures. Pre-training phase involves training the model on huge textual data, which is the  │\n",
              "│ stage where the model learns and understands the language. Google used multiple TPUs to pre-train a BERT model, │\n",
              "│ which took 4 days to train on such a large infrastructure. After training, the model can be reused for various  │\n",
              "│ tasks in NLP. There are three steps involved in pre-training BERT: Text corpus selection, Masked Language       │\n",
              "│ Modeling, Next Sentence Prediction.                                                                             │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:01:28\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m313227\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:01:28.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">313227</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Entire Summarization lenght: \u001b[1;95m1455\u001b[0m/\u001b[1;36m2860\u001b[0m character\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entire Summarization lenght: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">1455</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2860</span> character\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summarization reduction: \u001b[1;95m50.9\u001b[0m\u001b[1;95m%\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summarization reduction: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">50.9</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">%</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(post_summary15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "UdOpxXKLDpZh",
        "outputId": "1c04f8ec-692e-4d0e-f373-42a835725942"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " BERT: A Beginner-Friendly Explanation by Pushpam Punjabi explains how a machine understands language through \n",
              "language models, which are essential components of Natural Language Processing \u001b[1m(\u001b[0mNLP\u001b[1m)\u001b[0m designed to understand and \n",
              "generate human language. They use statistical and machine learning techniques to analyze and learn from large \n",
              "amounts of text data, enabling them to identify patterns and relationships between words, phrases, and sentences. \n",
              "Language models have revolutionized NLP and have surpassed humans in some of the tasks in NLP. The blog discusses \n",
              "the Bi-directional Encoder Representations from Transformers \u001b[1m(\u001b[0mBERT\u001b[1m)\u001b[0m, a language model developed by Google in \u001b[1;36m2018\u001b[0m. \n",
              "The model is trained on massive amounts of text data to understand the context and meaning of words and phrases in \n",
              "a sentence. BERT uses transformer deep learning architecture to process information bidirectionally, allowing it to\n",
              "better understand the nuances of language, including idioms, sarcasm, and complex sentence structures. Pre-training\n",
              "phase involves training the model on huge textual data, which is the stage where the model learns and understands \n",
              "the language. Google used multiple TPUs to pre-train a BERT model, which took \u001b[1;36m4\u001b[0m days to train on such a large \n",
              "infrastructure. After training, the model can be reused for various tasks in NLP. There are three steps involved in\n",
              "pre-training BERT: Text corpus selection, Masked Language Modeling, Next Sentence Prediction.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> BERT: A Beginner-Friendly Explanation by Pushpam Punjabi explains how a machine understands language through \n",
              "language models, which are essential components of Natural Language Processing <span style=\"font-weight: bold\">(</span>NLP<span style=\"font-weight: bold\">)</span> designed to understand and \n",
              "generate human language. They use statistical and machine learning techniques to analyze and learn from large \n",
              "amounts of text data, enabling them to identify patterns and relationships between words, phrases, and sentences. \n",
              "Language models have revolutionized NLP and have surpassed humans in some of the tasks in NLP. The blog discusses \n",
              "the Bi-directional Encoder Representations from Transformers <span style=\"font-weight: bold\">(</span>BERT<span style=\"font-weight: bold\">)</span>, a language model developed by Google in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span>. \n",
              "The model is trained on massive amounts of text data to understand the context and meaning of words and phrases in \n",
              "a sentence. BERT uses transformer deep learning architecture to process information bidirectionally, allowing it to\n",
              "better understand the nuances of language, including idioms, sarcasm, and complex sentence structures. Pre-training\n",
              "phase involves training the model on huge textual data, which is the stage where the model learns and understands \n",
              "the language. Google used multiple TPUs to pre-train a BERT model, which took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> days to train on such a large \n",
              "infrastructure. After training, the model can be reused for various tasks in NLP. There are three steps involved in\n",
              "pre-training BERT: Text corpus selection, Masked Language Modeling, Next Sentence Prediction.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐⭐⭐ BEST LangChain Summarization (text > 6000)"
      ],
      "metadata": {
        "id": "mWVzxh1VdeqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "#     SUMMARIZE TEXT WITH LANGCHAIN AND LAMINI LOCAL       #\n",
        "############################################################\n",
        "def AI_SummaryLangChain(checkpoint, text, chunks, overlap,method):\n",
        "  if method in ['stuff', 'map_reduce', 'refine']:\n",
        "    from langchain.llms import HuggingFacePipeline\n",
        "    from langchain.chains.summarize import load_summarize_chain\n",
        "    from langchain.chains.mapreduce import MapReduceChain\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.docstore.document import Document\n",
        "    \"\"\"\n",
        "    checkpoint is in the format of relative path\n",
        "    example:  checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "    text it is either a long string or a input long string or a loaded document into string\n",
        "    chunks: integer, lenght of the chunks splitting\n",
        "    ovelap: integer, overlap for cor attention and focus retreival\n",
        "    Got unsupported chain type: map-reduce. Should be one of dict_keys(['stuff', 'map_reduce', 'refine']\n",
        "    \"\"\"\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain.document_loaders import TextLoader\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = chunks,\n",
        "        chunk_overlap  = overlap,\n",
        "        length_function = len,\n",
        "    )\n",
        "    #IMPORTANT## Here we need a Document object from langchain.docstore.document\n",
        "    #Because the Chain requires PAGES elements\n",
        "    docs = text_splitter.create_documents([text])\n",
        "    #docs = [Document(page_content=t) for t in texts] #old method\n",
        "\n",
        "    #checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "    checkpoint = checkpoint\n",
        "    tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(checkpoint,\n",
        "                                                        device_map='auto',\n",
        "                                                        torch_dtype=torch.float32)\n",
        "    console.print(\"[yellow bold] Inizializing pipeline\")\n",
        "    ##############################################################################\n",
        "    #To use, you should have the transformers python package installed.\n",
        "    #Only supports text-generation, text2text-generation and summarization for now.\n",
        "    llm = HuggingFacePipeline.from_model_id(model_id=checkpoint,\n",
        "                                            task = 'text2text-generation',\n",
        "                                            model_kwargs={\"temperature\":0, \"max_length\":1000})\n",
        "\n",
        "    ###CHAIN TYPE FOR SUMMARIZATION Should be one of dict_keys(['stuff', 'map_reduce', 'refine']\n",
        "    chain = load_summarize_chain(llm, chain_type=method)\n",
        "    console.print(f\"[deep_pink4 bold] Entering Sumarization chain [italic]{method}[/italic] chain type\")\n",
        "    start = datetime.datetime.now() #not used now but useful\n",
        "    # taking from ipyWidget textarea content\n",
        "    full_summary = chain.run(docs)\n",
        "    stop = datetime.datetime.now() #not used now but useful\n",
        "    delta = stop-start\n",
        "    print(Panel(full_summary, title='AI Summarization'))\n",
        "    console.print(f\"[red bold]Summarization completed in {delta}\")\n",
        "    console.print(f\"Entire Summarization lenght: [bold bright_magenta]{len(full_summary)}[/bold bright_magenta]/{len(text)} character\")\n",
        "    reduction = '{:.1%}'.format(len(full_summary)/len(text))\n",
        "    console.print(f\"Summarization reduction: [bold bright_magenta]{reduction}\")\n",
        "    return full_summary\n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "rwxOrThudcld"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstractive Summarization 1"
      ],
      "metadata": {
        "id": "6fAtH4DiLbMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstractive_summary01 = AI_SummaryLangChain(LaMini,doc3,1500,300,'refine')  #refine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "6rA8OjQgLWol",
        "outputId": "0c71a443-0acd-4705-f599-78cceb0201eb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing pipeline\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing pipeline</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;125m Entering Sumarization chain \u001b[0m\u001b[1;3;38;5;125mrefine\u001b[0m\u001b[1;38;5;125m chain type\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold\"> Entering Sumarization chain </span><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold; font-style: italic\">refine</span><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold\"> chain type</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The blog discusses how language models are essential in NLP and how they can be used to understand human        │\n",
              "│ language. It explains how BERT is a language understanding model developed by Google in 2018. BERT is trained   │\n",
              "│ on massive amounts of text data to understand the context and meaning of words and phrases in a sentence. It    │\n",
              "│ uses transformer deep learning architecture to process information bidirectionally, allowing it to understand   │\n",
              "│ the context of a word based on both the words that come before and after it. Pre-training is expensive and      │\n",
              "│ Google used multiple TPUs to train a BERT model. Once the model understands the language, we can reuse it for   │\n",
              "│ various tasks in NLP. The blog provides more context to refine the existing summary.                            │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The blog discusses how language models are essential in NLP and how they can be used to understand human        │\n",
              "│ language. It explains how BERT is a language understanding model developed by Google in 2018. BERT is trained   │\n",
              "│ on massive amounts of text data to understand the context and meaning of words and phrases in a sentence. It    │\n",
              "│ uses transformer deep learning architecture to process information bidirectionally, allowing it to understand   │\n",
              "│ the context of a word based on both the words that come before and after it. Pre-training is expensive and      │\n",
              "│ Google used multiple TPUs to train a BERT model. Once the model understands the language, we can reuse it for   │\n",
              "│ various tasks in NLP. The blog provides more context to refine the existing summary.                            │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:58\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m899842\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:58.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">899842</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Entire Summarization lenght: \u001b[1;95m735\u001b[0m/\u001b[1;36m2860\u001b[0m character\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entire Summarization lenght: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">735</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2860</span> character\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summarization reduction: \u001b[1;95m25.7\u001b[0m\u001b[1;95m%\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summarization reduction: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">25.7</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">%</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abstractive_summary01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "rCNk98ApLeXM",
        "outputId": "a13c2d03-0800-4ee7-a1d2-afc32c74b2c9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The blog discusses how language models are essential in NLP and how they can be used to understand human language. \n",
              "It explains how BERT is a language understanding model developed by Google in \u001b[1;36m2018\u001b[0m. BERT is trained on massive \n",
              "amounts of text data to understand the context and meaning of words and phrases in a sentence. It uses transformer \n",
              "deep learning architecture to process information bidirectionally, allowing it to understand the context of a word \n",
              "based on both the words that come before and after it. Pre-training is expensive and Google used multiple TPUs to \n",
              "train a BERT model. Once the model understands the language, we can reuse it for various tasks in NLP. The blog \n",
              "provides more context to refine the existing summary.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The blog discusses how language models are essential in NLP and how they can be used to understand human language. \n",
              "It explains how BERT is a language understanding model developed by Google in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span>. BERT is trained on massive \n",
              "amounts of text data to understand the context and meaning of words and phrases in a sentence. It uses transformer \n",
              "deep learning architecture to process information bidirectionally, allowing it to understand the context of a word \n",
              "based on both the words that come before and after it. Pre-training is expensive and Google used multiple TPUs to \n",
              "train a BERT model. Once the model understands the language, we can reuse it for various tasks in NLP. The blog \n",
              "provides more context to refine the existing summary.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstractive Summarization 2"
      ],
      "metadata": {
        "id": "bMdaSx1gLe3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstractive_summary02 = AI_SummaryLangChain(LaMini,doc3,1500,300,'map_reduce')  #map_reduce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "TB1bU1y8LiVE",
        "outputId": "39e65071-f766-4738-8e73-0b49d158bf87"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing pipeline\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing pipeline</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;125m Entering Sumarization chain \u001b[0m\u001b[1;3;38;5;125mmap_reduce\u001b[0m\u001b[1;38;5;125m chain type\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold\"> Entering Sumarization chain </span><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold; font-style: italic\">map_reduce</span><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold\"> chain type</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The blog discusses the importance of language models in NLP and how they can be used to understand human        │\n",
              "│ language. The BERT model is a language understanding model that is trained on text data to understand the       │\n",
              "│ context and meaning of words and phrases in a sentence. It uses transformer deep learning architecture to       │\n",
              "│ process information bidirectionally and can be reused for various tasks in NLP. The article also discusses the  │\n",
              "│ process of selecting text corpus and predicting the next sentence prediction for a simulated language model.    │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The blog discusses the importance of language models in NLP and how they can be used to understand human        │\n",
              "│ language. The BERT model is a language understanding model that is trained on text data to understand the       │\n",
              "│ context and meaning of words and phrases in a sentence. It uses transformer deep learning architecture to       │\n",
              "│ process information bidirectionally and can be reused for various tasks in NLP. The article also discusses the  │\n",
              "│ process of selecting text corpus and predicting the next sentence prediction for a simulated language model.    │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:52\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m058032\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:52.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">058032</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Entire Summarization lenght: \u001b[1;95m536\u001b[0m/\u001b[1;36m2860\u001b[0m character\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entire Summarization lenght: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">536</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2860</span> character\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summarization reduction: \u001b[1;95m18.7\u001b[0m\u001b[1;95m%\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summarization reduction: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">18.7</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">%</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abstractive_summary02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "EPEUBp2VVE7t",
        "outputId": "77e813ba-f39d-4c81-aa1c-5c3bafc8fb1e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The blog discusses the importance of language models in NLP and how they can be used to understand human language. \n",
              "The BERT model is a language understanding model that is trained on text data to understand the context and meaning\n",
              "of words and phrases in a sentence. It uses transformer deep learning architecture to process information \n",
              "bidirectionally and can be reused for various tasks in NLP. The article also discusses the process of selecting \n",
              "text corpus and predicting the next sentence prediction for a simulated language model.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The blog discusses the importance of language models in NLP and how they can be used to understand human language. \n",
              "The BERT model is a language understanding model that is trained on text data to understand the context and meaning\n",
              "of words and phrases in a sentence. It uses transformer deep learning architecture to process information \n",
              "bidirectionally and can be reused for various tasks in NLP. The article also discusses the process of selecting \n",
              "text corpus and predicting the next sentence prediction for a simulated language model.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstractive Summarization 3"
      ],
      "metadata": {
        "id": "WhmF7oDaVg0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstractive_summary03 = AI_SummaryLangChain(LaMini,doc3,1500,300,'stuff')  #stuff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "MXTYb6BiLiKE",
        "outputId": "109423bf-3ab6-4215-e54d-91bda27ca734"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing pipeline\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing pipeline</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;125m Entering Sumarization chain \u001b[0m\u001b[1;3;38;5;125mstuff\u001b[0m\u001b[1;38;5;125m chain type\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold\"> Entering Sumarization chain </span><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold; font-style: italic\">stuff</span><span style=\"color: #af005f; text-decoration-color: #af005f; font-weight: bold\"> chain type</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The blog discusses how language models are used in Natural Language Processing (NLP) to understand and generate │\n",
              "│ human language. The blog also discusses Bi-directional Encoder Representations from Transformers (BERT), which  │\n",
              "│ is one of the biggest milestones in the field. The blog also discusses how to train BERT, a language            │\n",
              "│ understanding model developed by Google in 2018, using a large amount of textual data. The model is trained on  │\n",
              "│ a multifaceted architecture that allows it to process information bidirectionally, allowing it to better        │\n",
              "│ understand the nuances of language. The blog also mentions how to reuse the model for various tasks in NLP.     │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The blog discusses how language models are used in Natural Language Processing (NLP) to understand and generate │\n",
              "│ human language. The blog also discusses Bi-directional Encoder Representations from Transformers (BERT), which  │\n",
              "│ is one of the biggest milestones in the field. The blog also discusses how to train BERT, a language            │\n",
              "│ understanding model developed by Google in 2018, using a large amount of textual data. The model is trained on  │\n",
              "│ a multifaceted architecture that allows it to process information bidirectionally, allowing it to better        │\n",
              "│ understand the nuances of language. The blog also mentions how to reuse the model for various tasks in NLP.     │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:26\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m499152\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:26.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">499152</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Entire Summarization lenght: \u001b[1;95m647\u001b[0m/\u001b[1;36m2860\u001b[0m character\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Entire Summarization lenght: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">647</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2860</span> character\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summarization reduction: \u001b[1;95m22.6\u001b[0m\u001b[1;95m%\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summarization reduction: <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">22.6</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">%</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abstractive_summary03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "lzgoV7ZPVHiw",
        "outputId": "8990059b-ee19-4aed-c188-afbe2a6f4d5d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The blog discusses how language models are used in Natural Language Processing \u001b[1m(\u001b[0mNLP\u001b[1m)\u001b[0m to understand and generate \n",
              "human language. The blog also discusses Bi-directional Encoder Representations from Transformers \u001b[1m(\u001b[0mBERT\u001b[1m)\u001b[0m, which is \n",
              "one of the biggest milestones in the field. The blog also discusses how to train BERT, a language understanding \n",
              "model developed by Google in \u001b[1;36m2018\u001b[0m, using a large amount of textual data. The model is trained on a multifaceted \n",
              "architecture that allows it to process information bidirectionally, allowing it to better understand the nuances of\n",
              "language. The blog also mentions how to reuse the model for various tasks in NLP.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The blog discusses how language models are used in Natural Language Processing <span style=\"font-weight: bold\">(</span>NLP<span style=\"font-weight: bold\">)</span> to understand and generate \n",
              "human language. The blog also discusses Bi-directional Encoder Representations from Transformers <span style=\"font-weight: bold\">(</span>BERT<span style=\"font-weight: bold\">)</span>, which is \n",
              "one of the biggest milestones in the field. The blog also discusses how to train BERT, a language understanding \n",
              "model developed by Google in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span>, using a large amount of textual data. The model is trained on a multifaceted \n",
              "architecture that allows it to process information bidirectionally, allowing it to better understand the nuances of\n",
              "language. The blog also mentions how to reuse the model for various tasks in NLP.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "========//==============//==============//============="
      ],
      "metadata": {
        "id": "Bo9Q6uxALjMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions\n",
        "- Load the file into a `doc` variable\n",
        "- run AI_SummaryPL (3700,500) for long Summary\n",
        "- run AI_SummaryLangChain (1500,300,'refine') for shorter one"
      ],
      "metadata": {
        "id": "XBQkEL4XQEtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################### Import TEXT file as String ###################\n",
        "fname = '/content/nlp-basics-abstractive-and-extractive-text-summarization.txt'\n",
        "with open(fname) as f:\n",
        "    doc3 = f.read()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "TEL4gBoHPBw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}